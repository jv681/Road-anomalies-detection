# üöó Real-Time Road Anomaly Detection using Edge AI on Raspberry Pi

<div align="center">

![ARM Edge AI](https://img.shields.io/badge/ARM-Edge%20AI%20Competition-0091BD?style=for-the-badge&logo=arm&logoColor=white)
![Raspberry Pi](https://img.shields.io/badge/Raspberry%20Pi%204-A22846?style=for-the-badge&logo=raspberrypi&logoColor=white)
![YOLOv5](https://img.shields.io/badge/YOLOv5-Object%20Detection-FF6F00?style=for-the-badge&logo=pytorch&logoColor=white)
![ONNX](https://img.shields.io/badge/ONNX%20Runtime-Edge%20Inference-005CED?style=for-the-badge&logo=onnx&logoColor=white)
![Python](https://img.shields.io/badge/Python-3.8+-3776AB?style=for-the-badge&logo=python&logoColor=white)
![OpenCV](https://img.shields.io/badge/OpenCV-Vision-5C3EE8?style=for-the-badge&logo=opencv&logoColor=white)
![License](https://img.shields.io/badge/License-MIT-green?style=for-the-badge)

**An ARM Edge AI competition submission ‚Äî deploying a custom-trained YOLOv5s model on Raspberry Pi 4 for real-time road anomaly detection with GPS-integrated logging.**

[Features](#-features) ‚Ä¢ [Architecture](#-system-architecture) ‚Ä¢ [Setup](#-installation) ‚Ä¢ [Usage](#-how-to-run) ‚Ä¢ [Results](#-results--performance-metrics) ‚Ä¢ [Author](#-author)

</div>

---

## üìå Project Overview

Road anomalies such as **potholes** and **obstacles** (barriers, debris, fallen objects) are a major cause of vehicle damage and road accidents. This project brings intelligent road safety to the edge by deploying a custom-trained lightweight **YOLOv5s** model on a **Raspberry Pi 4**, enabling real-time anomaly detection without any cloud dependency.

The system is designed for two operational modes:

| Mode | Description |
|------|-------------|
| üé• **Offline Video Inference** | Process dashcam footage or pre-recorded videos (`test2.py`) |
| üì∑ **Real-Time Camera Inference** | Live detection via USB webcam or Pi Camera (`test.py`) |

Every detected anomaly is automatically logged with a **timestamp**, **bounding box**, **confidence score**, **snapshot image**, and **GPS coordinates** ‚Äî making it fully suitable for fleet management, road monitoring, and smart city applications.

---

## ‚ú® Features

- ‚úÖ **Real-time anomaly detection** directly on Raspberry Pi 4
- ‚úÖ **Dual inference modes** ‚Äî offline video and live camera feed
- ‚úÖ **Pothole & obstacle detection** (barriers, debris, fallen objects)
- ‚úÖ **Automatic snapshot capture** for every detected anomaly
- ‚úÖ **Structured CSV logging** with timestamps and confidence scores
- ‚úÖ **GPS coordinate integration** for geo-tagged anomaly reporting
- ‚úÖ **Lightweight ONNX Runtime inference** ‚Äî no GPU required
- ‚úÖ **Custom dataset** built and annotated with Roboflow
- ‚úÖ **Optimized edge deployment** pipeline with Non-Maximum Suppression
- ‚úÖ **Robust detection** across varied road conditions and lighting

---

## üèóÔ∏è System Architecture

```mermaid
graph TB
    subgraph Data["üìä Data Layer"]
        A[üåê Internet Images] --> C[Roboflow Annotation Platform]
        B[üõ£Ô∏è Real-World Road Images] --> C
        C --> D[Custom YOLO Dataset]
    end

    subgraph Training["üß† Training Layer - Google Colab"]
        D --> E[YOLOv5s Training - GPU]
        E --> F[best.pt Model]
        F --> G[ONNX Export & Optimization]
        G --> H[best.onnx]
    end

    subgraph Edge["‚ö° Edge Deployment - Raspberry Pi 4"]
        H --> I[ONNX Runtime Engine]
        J[üì∑ USB / Pi Camera] --> K[Frame Capture]
        L[üé• Dashcam Video] --> K
        K --> M[Frame Preprocessing]
        M --> I
        I --> N[NMS Post-processing]
        N --> O[Bounding Box Rendering]
    end

    subgraph Output["üìÅ Output Layer"]
        O --> P[üñºÔ∏è Snapshot Image]
        O --> Q[üìù CSV Log Entry]
        O --> R[üõ∞Ô∏è GPS Coordinate Tag]
        P & Q & R --> S[outputs/ Directory]
    end

    style Data fill:#1a1a2e,stroke:#0f3460,color:#eee
    style Training fill:#16213e,stroke:#0f3460,color:#eee
    style Edge fill:#0f3460,stroke:#533483,color:#eee
    style Output fill:#533483,stroke:#e94560,color:#eee
```

---

## üîÑ Inference Pipeline Flowchart

```mermaid
flowchart TD
    A([‚ñ∂ Start]) --> B{Input Source?}
    B -->|Video File| C[Load Video File<br/>videos/test.mp4]
    B -->|Live Camera| D[Open Camera Stream<br/>USB / Pi Camera]

    C --> E[Read Frame]
    D --> E

    E --> F[Preprocess Frame<br/>Resize ‚Üí 640√ó640<br/>Normalize ‚Üí 0-1<br/>Add Batch Dim]

    F --> G[ONNX Runtime Inference<br/>model/best.onnx]

    G --> H[Raw Predictions Output]

    H --> I[Apply NMS<br/>conf_thresh: 0.4<br/>iou_thresh: 0.45]

    I --> J{Anomaly<br/>Detected?}

    J -->|No| K[Display Clean Frame]
    J -->|Yes| L[Draw Bounding Box<br/>+ Class Label<br/>+ Confidence Score]

    L --> M[Capture Snapshot<br/>outputs/snapshots/]
    L --> N[Log to CSV<br/>outputs/anomaly_log.csv]
    L --> O[Tag GPS Coordinates]

    M & N & O --> P[Display Annotated Frame]
    K --> P

    P --> Q{More Frames?}
    Q -->|Yes| E
    Q -->|No| R([‚èπ End])

    style A fill:#2ecc71,color:#000
    style R fill:#e74c3c,color:#fff
    style G fill:#3498db,color:#fff
    style J fill:#f39c12,color:#000
    style I fill:#9b59b6,color:#fff
```

---

## üöÄ Deployment Pipeline

```mermaid
graph LR
    A[üñ•Ô∏è Google Colab<br/>Model Training] -->|best.pt| B[‚öôÔ∏è ONNX Export<br/>torch.onnx.export]
    B -->|best.onnx| C[üì¶ Transfer to<br/>Raspberry Pi]
    C --> D[üêç Python Venv<br/>Setup]
    D --> E[üì¶ Install Dependencies<br/>onnxruntime ¬∑ opencv ¬∑ numpy]
    E --> F[‚ñ∂Ô∏è Run Inference<br/>test.py / test2.py]
    F --> G[üìä Outputs<br/>CSV ¬∑ Snapshots ¬∑ GPS]

    style A fill:#34495e,color:#fff
    style B fill:#2980b9,color:#fff
    style C fill:#27ae60,color:#fff
    style D fill:#8e44ad,color:#fff
    style E fill:#e67e22,color:#fff
    style F fill:#c0392b,color:#fff
    style G fill:#16a085,color:#fff
```

---

## üìÇ File Structure

```
ARM-PROJECT/
‚îÇ
‚îú‚îÄ‚îÄ üìÅ model/
‚îÇ   ‚îî‚îÄ‚îÄ best.onnx                  # Exported ONNX model (YOLOv5s)
‚îÇ
‚îú‚îÄ‚îÄ üìÅ outputs/
‚îÇ   ‚îú‚îÄ‚îÄ anomaly_log.csv            # Auto-generated anomaly log
‚îÇ   ‚îî‚îÄ‚îÄ üìÅ snapshots/             # Captured anomaly frames
‚îÇ       ‚îú‚îÄ‚îÄ pothole_20240515_143201.jpg
‚îÇ       ‚îî‚îÄ‚îÄ obstacle_20240515_143512.jpg
‚îÇ
‚îú‚îÄ‚îÄ üìÅ videos/
‚îÇ   ‚îî‚îÄ‚îÄ test.mp4                   # Sample test video for offline inference
‚îÇ
‚îú‚îÄ‚îÄ üìÅ venv/                       # Python virtual environment
‚îÇ
‚îú‚îÄ‚îÄ üìÅ docs/                       # Documentation assets (to be added)
‚îÇ   ‚îú‚îÄ‚îÄ dataset.png                # Roboflow annotation screenshot
‚îÇ   ‚îî‚îÄ‚îÄ  output.png                 # Sample detection output
‚îÇ   
‚îÇ
‚îú‚îÄ‚îÄ training_report.pdf        # Google Colab training documentation
‚îú‚îÄ‚îÄ test.py                        # üé• Real-time webcam inference script
‚îú‚îÄ‚îÄ test2.py                       # üìº Offline video inference script
‚îú‚îÄ‚îÄ README.md                      # This file
‚îî‚îÄ‚îÄ requirements.txt               # Python dependencies
```

---

## üìä Dataset Creation

The dataset was created from scratch using a combination of internet-sourced road images and real-world photographs.

### Annotation Platform: Roboflow

- **Tool:** [Roboflow](https://roboflow.com) ‚Äî bounding box annotation
- **Classes:** `pothole`, `obstacle`
- **Augmentations applied:** flip, rotation, brightness adjustment, mosaic
- **Export format:** YOLOv5 PyTorch format

![Dataset Screenshot](docs/dataset.png)

> *Screenshot of Roboflow annotation interface showing labeled pothole and obstacle classes*

[Downlaod Dataset](https://universe.roboflow.com/testing-f6dvv/road_anomalies-3f2b3/browse?queryText=&pageSize=50&startingIndex=0&browseQuery=true)



### Dataset Summary

| Property | Value |
|----------|-------|
| Total Images | ~1000+ annotated frames |
| Classes | `pothole`, `obstacle` |
| Annotation Tool | Roboflow |
| Export Format | YOLOv5 PyTorch |
| Train / Val Split | 80% / 20% |
| Augmentation | Flip, Rotation |

---
<p align="center">
  <img src="docs/6.png" width="48%">
</p>

> *Dataset*

## üß† Model Training

Training was performed on **Google Colab** using a free GPU runtime for fast iteration.

### Training Configuration

| Parameter | Value |
|-----------|-------|
| Base Model | YOLOv5s (Small) |
| Training Platform | Google Colab (GPU) |
| Epochs | 100 |
| Image Size | 640 √ó 640 |
| Batch Size | 16 |
| Optimizer | SGD |
| Learning Rate | 0.01 |

### Training Metrics

| Metric | Value |
|--------|-------|
| üéØ Precision | ~0.88 |
| üîÅ Recall | ~0.75 |
| üìà mAP@50 | ~0.71 |
| üìâ Box Loss | Converged |
| üìâ Object Loss | Converged |

> Training documentation PDF from Google Colab available in `docs/training_report.pdf`

---

## ‚öôÔ∏è Model Optimization

The trained PyTorch model was exported to **ONNX format** for hardware-agnostic, optimized edge inference.

```bash
# Export from YOLOv5 training environment
python export.py --weights best.pt --include onnx --img 640
```

### Why ONNX Runtime?

| Feature | Benefit |
|---------|---------|
| ‚úÖ No PyTorch on Pi | Eliminates heavy ML framework dependency |
| ‚úÖ ARM Optimized | Efficient inference on ARM Cortex-A72 |
| ‚úÖ Faster Startup | Reduced initialization time |
| ‚úÖ Cross-Platform | Consistent behavior across environments |
| ‚úÖ Quantization Ready | Supports INT8 optimization (future scope) |

---

## üçì Deployment on Raspberry Pi

### Hardware Setup

| Component | Specification |
|-----------|--------------|
| Board | Raspberry Pi 4 Model B with heat sink (8GB RAM) |
| Cooling | Heat sink attached (thermal management) |
| Camera | USB Webcam / Raspberry Pi Camera Module |
| Storage | 32GB+ microSD Card |
| OS | Raspberry Pi OS (64-bit) |
| Power | 5V 3A USB-C supply |

<p align="center">
  <img src="docs/rpi_with heatsink.jpeg" width="48%">
  <img src="docs/setup.png" width="48%">
</p>
> ‚ö†Ô∏è **Note:** A heat sink is strongly recommended for sustained inference workloads to prevent thermal throttling on the Raspberry Pi 4.

---

## üõ†Ô∏è Installation

### Prerequisites

- Raspberry Pi 4 with Raspberry Pi OS
- Python 3.8+
- USB Webcam(1080p)
- Internet connection (for initial setup)

### Step 1: Clone the Repository

```bash
git clone https://github.com/jv681/ARM-PROJECT.git
cd ARM-PROJECT
```

### Step 2: Create Virtual Environment

```bash
python3 -m venv venv
source venv/bin/activate
```

### Step 3: Install Dependencies

```bash
pip install --upgrade pip
pip install opencv-python-headless numpy onnxruntime
```

Or using requirements file:

```bash
pip install -r requirements.txt
```

### `requirements.txt`

```
opencv-python-headless>=4.5.0
numpy>=1.21.0
onnxruntime>=1.12.0
```

> For GPS support, additionally install:
> ```bash
> pip install gpsd-py3
> ```

---

## ‚ñ∂Ô∏è How to Run

### üé• Video Inference (Offline Mode)

Process a pre-recorded dashcam video:

```bash
# Activate virtual environment
source venv/bin/activate

# Run video inference
python test2.py
```

**Configuration inside `test2.py`:**

```python
VIDEO_PATH = "videos/test.mp4"       # Path to input video
MODEL_PATH = "model/best.onnx"       # ONNX model path
CONF_THRESHOLD = 0.4                 # Confidence threshold
IOU_THRESHOLD = 0.45                 # NMS IoU threshold
OUTPUT_DIR = "outputs/"              # Output directory
```

---

### üì∑ Real-Time Camera Inference

Perform live detection using connected camera:

```bash
# Activate virtual environment
source venv/bin/activate

# Run real-time inference
python test.py
```

**Configuration inside `test.py`:**

```python
CAMERA_INDEX = 0                     # 0 = USB cam, 1 = Pi Camera
MODEL_PATH = "model/best.onnx"
CONF_THRESHOLD = 0.4
IOU_THRESHOLD = 0.45
OUTPUT_DIR = "outputs/"
```

**Controls during runtime:**

| Key | Action |
|-----|--------|
| `q` | Quit inference |
| `s` | Save current frame manually |

---

## üñºÔ∏è Output Examples

### Detection Output

## Output

<p align="center">
  <img src="docs/1.jpeg" width="45%">
  <img src="docs/2.jpeg" width="45%">
</p>

<p align="center">
  <img src="docs/3.jpeg" width="45%">
  <img src="docs/4.jpeg" width="45%">
</p>

> *Sample detection showing pothole bounding box with confidence score overlay*

<p align="center">
  <img src="docs/5.png" width="600">
</p>

> *Log file with timestamp, gps(lat and long), prediction score*

### Annotated Frame Example

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                     ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                      ‚îÇ
‚îÇ   ‚îÇ pothole  ‚îÇ  conf: 0.87          ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                      ‚îÇ
‚îÇ                                     ‚îÇ
‚îÇ        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îÇ
‚îÇ        ‚îÇ   obstacle     ‚îÇ conf:0.73 ‚îÇ
‚îÇ        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìù CSV Logging Format

All detected anomalies are automatically appended to `outputs/anomaly_log.csv`:

```csv
timestamp,class,confidence,x1,y1,x2,y2,snapshot_path,latitude,longitude
2024-05-15 14:32:01,pothole,0.87,142,310,298,420,outputs/snapshots/pothole_143201.jpg,12.9716,77.5946
2024-05-15 14:35:12,obstacle,0.73,400,200,580,350,outputs/snapshots/obstacle_143512.jpg,12.9720,77.5950
```

| Column | Description |
|--------|-------------|
| `timestamp` | Detection date and time |
| `class` | Detected class (`pothole` / `obstacle`) |
| `confidence` | Model confidence score (0.0 ‚Äì 1.0) |
| `x1, y1, x2, y2` | Bounding box pixel coordinates |
| `snapshot_path` | Relative path to saved snapshot image |
| `latitude` | GPS latitude at detection |
| `longitude` | GPS longitude at detection |

---

## üõ∞Ô∏è GPS Logging

The system supports real GPS integration using a **USB GPS module** connected to the Raspberry Pi.

### GPS Setup

```bash
# Install GPS daemon
sudo apt-get install gpsd gpsd-clients

# Configure GPS device
sudo gpsd /dev/ttyUSB0 -F /var/run/gpsd.sock

# Install Python GPS library
pip install gpsd-py3
```

### GPS Integration in Code

```python
import gpsd

def get_gps_coordinates():
    try:
        gpsd.connect()
        packet = gpsd.get_current()
        return packet.lat, packet.lon
    except Exception:
        return None, None  # Fallback if GPS unavailable
```

> üìç If no GPS module is connected, coordinates default to `None` and can be filled in post-processing using video timestamps.

---

## üìà Results & Performance Metrics

### Model Performance

| Metric | Value |
|--------|-------|
| üéØ Precision | **~0.88** |
| üîÅ Recall | **~0.75** |
| üìä mAP@50 | **~0.71** |
| üßÆ Model Size | ~14 MB (ONNX) |
| üè∑Ô∏è Classes | pothole, obstacle |

### Edge Deployment Performance

| Metric | Raspberry Pi 4 |
|--------|---------------|
| ‚ö° Inference Latency | ~350‚Äì500 ms/frame |
| üéûÔ∏è Effective FPS | >5 FPS |
| üß† RAM Usage | ~300‚Äì400 MB |
| üå°Ô∏è CPU Temp (with heatsink) | ~55‚Äì65¬∞C |
| üì¶ Runtime | ONNX Runtime (CPU) |
| üñ•Ô∏è Deployment OS | Raspberry Pi OS 64-bit |

> üí° **Note:** FPS can be improved through resolution downscaling, frame skipping, or future quantization (INT8) of the ONNX model.

### Detection Accuracy by Class

| Class | Precision | Recall | mAP@50 |
|-------|-----------|--------|--------|
| pothole | 0.91 | 0.78 | 0.74 |
| obstacle | 0.85 | 0.72 | 0.68 |
| **Overall** | **0.88** | **0.75** | **0.71** |

---

## üèÜ Key Achievements

- ü•á Successfully deployed a **custom-trained YOLOv5 model** on ARM hardware (Raspberry Pi 4)
- üèóÔ∏è Built a **complete end-to-end pipeline** ‚Äî from dataset creation to edge deployment
- üì° Integrated **real GPS coordinate logging** for geo-referenced anomaly mapping
- üóÇÔ∏è Implemented **structured CSV logging** for fleet and infrastructure management use cases
- üì∏ Automated **snapshot capture** system for evidence-based anomaly reporting
- ‚öôÔ∏è Achieved **stable real-time inference** under sustained thermal load using heat sink cooling
- üì¶ Eliminated cloud dependency with **fully offline on-device inference**
- üõ£Ô∏è Demonstrated **robust detection** across varied road conditions, lighting, and camera angles

---

## üîÆ Future Improvements

| Improvement | Description |
|-------------|-------------|
| üöÄ INT8 Quantization | Reduce model size and improve FPS via ONNX INT8 quantization |
| üîß TensorRT / TFLite | Explore further optimization with platform-specific runtimes |
| üì± Mobile App | Build companion app for real-time monitoring dashboard |
| üó∫Ô∏è Heatmap Generation | Generate road anomaly heatmaps from GPS-logged data |
| ‚òÅÔ∏è Cloud Sync | Optional upload of logs to cloud for fleet-wide monitoring |
| üéØ Model Improvement | Expand dataset and improve mAP with data augmentation |
| üîã Power Optimization | Explore duty-cycle inference for battery-powered deployments |
| üöó OBD Integration | Correlate anomaly detection with vehicle speed via OBD-II |

---

## üìö References

- [YOLOv5 by Ultralytics](https://github.com/ultralytics/yolov5)
- [ONNX Runtime Documentation](https://onnxruntime.ai/docs/)
- [Roboflow ‚Äî Dataset Annotation Platform](https://roboflow.com)
- [OpenCV Python Documentation](https://docs.opencv.org/4.x/d6/d00/tutorial_py_root.html)
- [Raspberry Pi Official Documentation](https://www.raspberrypi.com/documentation/)
- [gpsd Python Library](https://pypi.org/project/gpsd-py3/)
- [ARM Developer ‚Äî Edge AI Resources](https://developer.arm.com/solutions/machine-learning-on-arm)

---

## üë§ Author

<div align="center">

**DhinekkaB**

[![GitHub](https://img.shields.io/badge/GitHub-jv681-181717?style=for-the-badge&logo=github&logoColor=white)](https://github.com/jv681)

*ARM Edge AI Competition Submission*

*Real-Time Road Anomaly Detection using Edge AI on Raspberry Pi*

</div>

---

## üìÑ License

```
MIT License

Copyright (c) 2024 DhinekkaB

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
```

---

<div align="center">

‚≠ê **Star this repo if you found it useful!** ‚≠ê

*Built with ‚ù§Ô∏è for safer roads using Edge AI*

![ARM](https://img.shields.io/badge/Powered%20by-ARM%20Cortex--A72-0091BD?style=flat-square&logo=arm)
![Edge AI](https://img.shields.io/badge/Edge%20AI-On%20Device-success?style=flat-square)
![Made in India](https://img.shields.io/badge/Made%20in-India%20üáÆüá≥-orange?style=flat-square)

</div>
